Things to do in the acsint / acsbridge world.
Some of these tasks make the system more usable;
some simply make the software more palatable to the linux community,
in hopes of submitting it to the kernel.
These are in no particular order.

* Follow the linux coding standards and recommendations,
at least for the two kernel modules.
We can postpone this for now, but must address it before submission.
I tend to tweak comments and variables if I am changing a section
of code for some other reason;
but I don't generally make changes just for pretty sake - not yet anyways.
I think we should aim for this summer to submit the modules
for consideration in the staging area.
It's a goal anyways.

* ttyclicks.c cannot sleep between clicks unless you patch the kernel.
The patch is included in this repository, but would not be accepted as is.
They would ask us to change the notifier system,
to pass back a new action code like NOTIFY_SLEEP or some such,
and I'm not sure they would even accept that.
"It's too intrusive, and nobody else wants it."
I use to think we get our infrastructure accepted into the linux core,
and then submit properly written modules using that structure.
But it's hard to get changes accepted in the heart of the kernel.
So now I think you submit functioning modules into staging,
then say to the community,
"That's kind of an ugly implementation.
These things should really be here in the core.
Here is a patch that fixes it."
Get your foot in the door first, with a module that is perhaps
suboptimal, then, when they are stuck with it,
and they have to deal with it, propose changes to the kernel
itself that makes the entire system better.
So with this mindset, I suggest we submit ttyclicks
without the sleep function; it just spins cpu cycles to do its timing,
and then, when it is part of the kernel, we point out how silly that is
and suggest a change to the notifier system.
The same thoughts apply to the next item.

* Clicks, swoops, and notes should really be handled in keyboard.c,
along side the existing function kd_mksound().
I would add kd_mkclick() kd_mkswoop() and kd_mknotes(),
and then the ttyclicks module would use these functions,
and other modules could use them as well.
This is obviously a better approach,
a centralized suite of functions that generates sounds
through the pc speaker or equivalent.
Obviously better, but we'd have a hard time getting it approved.
So again, back to my reverse thinking -
we submit ttyclicks as is, generating its own noises,
and once it is in staging we point out how silly that design is,
and submit a patch that moves the sound functions back to keyboard.c
where they belong.

* ttyclicks makes a quick high beep for echoed capital letters.
this is ascii, and not international.
Capital n tilde, in Spanish, won't beep properly.
This is probably not vital, but we might want to look at it some day.

* Should we have a module parameter to vary the speed
of the clicks in ttyclicks.c?
It would be course, perhaps one of 5 levels, but better than nothing.

* Convert acsint.c to unicodes.
I'm sure it won't be accepted without this change,
and it's a good idea anyways.
We have to support speech in other languages.
Linux applications crank out utf8,
and on some older boxes, iso8859.
This is converted by vt.c to unicodes.
And thus, unicodes are passed to acsint by the vt notifier.
(I need to verify this; I hope it's true.)
I could store unicodes in an array of ints, instead of ascii
or 8859-x in an array of unsigned chars.
I could then pass these ints down to the bridge.
The bridge can continue to convert them back into chars for the application.
I'm not worried about that right now.
The first step is to make the acsint driver fully flexible,
so that it is accepted by the Linux community;
the bridge can catch up later.
I have already made all commands 4 bytes, so they line up with
4 byte unicodes in the pass-down stream.
We have also made buffers dynamic, allocated as needed,
which is even more important if we make those buffers 4 times as large,
to hold unicodes.

* The detection of echo in the acsint driver is bad.
It doesn't work for unicodes, only ascii,
and it presupposes a qwerty keyboard.
If you have another layout like dvorak it won't work at all.
I tried KBD_KEYSYM and that didn't seem to work at all.
Catching KBD_UNICODE events looks perfect, but my system doesn't
throw those events; I have no idea why.
This is an open issue.

* Same issue applies to acs_keystring() and acs_get1key() in the bridge.
These assume ascii qwerty, and won't work properly otherwise.

* Maintain a table of row mappings for screen memory.
Example, since last time you refreshed, row 7 has moved up to row 5.
This could happen if the screen scrolled up twice,
because two lines were printed.
But portions of the screen can scroll separately, and they can scroll down instead of up.
The vt notifier throws these scrolling events, and my Jupiter software
catches them, and moves your reading cursor accordingly.
(Perhaps speakup does too.)
An adapter, running in screen mode, needs this functionality,
and the acsint driver / bridge does not offer it today.
I am imagining an array of 25 ints,
telling where each line has moved to since our last refresh.
When this informatio passes down to the bridge,
it can move your reading cursor appropriately,
and any other markers that are tied to the text in screen memory.

* The bridge should track other "pointers" besides the reading cursor.
As text moves backwards in the tty buffer (line mode),
or scrolls up the screen (screen mode),
I keep the reading cursor and any speech index markers
in sync with the moving text.
You won't suddently hear a new sentence unrelated to the one you just heard.
But I should, perhaps, do the same for some user defined markers.
At least one, to implement cut&paste.
Mark the left edge of text, move your reading cursor to the right,
and when you hear the end of the desired block, issue a command
to capture text from the left marker to the reading cursor.
That left marker should track the text, in case any new text is generated.
But maybe you want to place half a dozen markers in your buffer.
After all, it's 50K, and that's a lot of realestate.
Perhaps I should maintain an array of 26 markers,
and tie them all to the moving text.
These might correspond to letters, the way you can mark
lines with letters in ed or in edbrowse.
I think the bridge should offer this functionality,
even if the adapters don't use it.

* Fold espeak into the bridge in a seamless fashion.
That is, set ss_style = SS_STYLE_ESPEAK,
and then the ss functions:
ss_shutup
ss_setvolume
ss_incspeed
ss_say_string
etc will just work.
This is more complicated than supporting various models over a serial port,
because it involves function calls to the espeak library,
a library that I would rather not load if it is not needed.
This can be done by conditional compilation, perhaps from a configure script
that defines ACS_ESPEAK as needed.
Another approach is to build an espeak synthesizer client
that makes all these espeak calls, but is a separate program.
It has the same interface as the DoubleTalk, for instance,
which is easy to program.
So the bridge would set ss-styel = SS_STYLE_DOUBLE,
but instead of talking to an external unit over a serial port,
it talks to an espeak synthesizer over pipes,
and the espeak synthesizer converts the doubletalk commands into espeak library calls.
I can't tell, at this point, which approach is better.
But I do know we need some way for an adapter to use all the ss functions
in the bridge, even when those functions control espeak or some other software
synth that is library based, rather than streaming commands
over a serial port or pipe or socket.

